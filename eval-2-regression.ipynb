{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import argparse\n",
    "import joblib\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import confusion_matrix, r2_score, mean_absolute_error, mean_squared_error, max_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_scorer(type=0):\n",
    "    ''' create scorer\n",
    "    Args:\n",
    "        type: 0=svc(binary),\n",
    "    Returns:\n",
    "        function: scorer(clf, X, y)\n",
    "    '''\n",
    "    # def binary_classification_scorer(clf, X, y):\n",
    "    #     y_pred = clf.predict(X)\n",
    "    #     cm = confusion_matrix(y, y_pred)\n",
    "    #     y_score = clf.decision_function(X)\n",
    "    #     auc = roc_auc_score(y, y_score)\n",
    "    #     return {'tn': cm[0, 0], 'fp': cm[0, 1],'fn': cm[1, 0], 'tp': cm[1, 1], 'auc': auc}\n",
    "\n",
    "    def binary_classification_scorer(clf, X, y):\n",
    "        y_pred = clf.predict(X)\n",
    "        cm = confusion_matrix(y, y_pred)\n",
    "        tn, fp, fn, tp = cm[0, 0], cm[0, 1], cm[1, 0], cm[1, 1]\n",
    "        accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        sensitivity = tp/(tp+fn)\n",
    "        specificity = tn/(tn+fp)\n",
    "        PPV = tp / (tp+fp)\n",
    "        NPV = tn / (fn+tn)\n",
    "        MCC = ((tp*tn)-(fp*fn))/math.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "\n",
    "        try:\n",
    "            y_score = clf.decision_function(X)\n",
    "            auc = roc_auc_score(y, y_score)\n",
    "        except Exception as e:\n",
    "            y_score = clf.predict_proba(X)\n",
    "            auc = roc_auc_score(y, y_score[:, 1])\n",
    "        \n",
    "        results = {\n",
    "            'tn':tn, \n",
    "            'fp':fp, \n",
    "            'fn':fn, \n",
    "            'tp':tp,\n",
    "            'auc':auc, \n",
    "            'accuracy':accuracy, \n",
    "            'precision':precision, \n",
    "            'sensitivity':sensitivity, \n",
    "            'specificity':specificity, \n",
    "            'MCC':MCC, \n",
    "            'PPV':PPV, \n",
    "            \"NPV\":NPV,\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def regression_scorer(clf, X, y):\n",
    "        y_pred = clf.predict(X)\n",
    "        # Evaluation: Training\n",
    "        size, var_num = X.shape\n",
    "        R2 = r2_score(y, y_pred)\n",
    "        adj_R2 = 1-(1-R2)*(size-1)/(size-var_num-1)\n",
    "        MAE = mean_absolute_error(y, y_pred)\n",
    "        MSE = mean_squared_error(y, y_pred)\n",
    "        RMSE = (MSE ** 0.5)\n",
    "        Max_error = max_error(y, y_pred)\n",
    "        \n",
    "        results = {\n",
    "            'R2':R2, \n",
    "            'adj_R2':adj_R2, \n",
    "            'MAE':MAE, \n",
    "            'MSE':MSE,\n",
    "            'RMSE':RMSE, \n",
    "            'Max_error':Max_error\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    if type==0: return binary_classification_scorer\n",
    "    elif type==1: return regression_scorer\n",
    "    # elif type==1: return binary_classification_scorer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從這邊修改設定與輸入檔案:\n",
    "- 測試資料\n",
    "- 模型\n",
    "- 挑選特徵\n",
    "- 輸出檔案\n",
    "- scaler\n",
    "---\n",
    "HW1 只要修改：\n",
    "- 模型\n",
    "- 挑選特徵\n",
    "- 輸出檔案\n",
    "\n",
    "**如果有人用 random forest 的模型先讓 ```auc = None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "problems = [0,1,2,3,4,5,6]\n",
    "\n",
    "# raw_df = pd.read_csv(\"./data/dialysis/Regression_2weeks_data_v2.csv\")\n",
    "raw_df = pd.read_csv(\"./hw2/dialysis-regression-14-ind-xday.csv\")\n",
    "\n",
    "model = joblib.load(\"./hw2/dialysis-regression-14-svr.model\")\n",
    "\n",
    "# features = pd.read_csv(\"./hw1/dialysis-binary-features.csv\")\n",
    "# features = list(features.columns)\n",
    "features = None\n",
    "\n",
    "outfile = \"./hw2/test-results.csv\"\n",
    "# outfile = \"./hw2/regression-svr-results.csv\"\n",
    "\n",
    "# if standardization\n",
    "scaler = joblib.load(\"./hw2/dialysis-regression-14-std.bin\")\n",
    "\n",
    "problem_type = 1 # 0: binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features number\n",
    "# features_num = len(features)\n",
    "# 重新命名欄位\n",
    "for idx, feature in enumerate(features):\n",
    "    if feature[0] == 'c': features[idx] = f'c{int(feature[1:])+39}'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw_df.drop(columns=['label'])\n",
    "if scaler: X = pd.DataFrame(scaler.transform(X), columns=X.columns) # Scale\n",
    "if features: X = X[features] # feature selection\n",
    "labels = raw_df.label\n",
    "# preds = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Azetry\\anaconda3\\envs\\ml-exp\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but SVR was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame.from_dict(overall_scorer(problem_type)(model, X, labels), orient='index').T\n",
    "# results = overall_scorer(problem_type)(model, X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save results:./hw2/test-results.csv\n"
     ]
    }
   ],
   "source": [
    "if outfile: \n",
    "    results_df.fillna(\"\").to_csv(outfile, index=False)\n",
    "    print(f\"Save results:{outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae0b0ae2c6da0805ef1653fbc63a03fd843bdd965278f8a5845e26c511ff151a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
